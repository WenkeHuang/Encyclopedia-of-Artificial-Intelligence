概率潜在语义分析（Probabilistic Latent Semantic Analysis，PLSA）是一种利用概率生成模型对文本集合进行话题分析的无监督学习方法。**模型最大特点是用隐变量表示话题。整个模型表示文本生成话题，话题生成单词。假设每个文本由一个话题分布决定，每个话题由一个单词分布决定。**

# 生成模型



生成模型中，单词变量![w](https://math.jianshu.com/math?formula=w)与文本变量![d](https://math.jianshu.com/math?formula=d)是观测变量，话题变量![z](https://math.jianshu.com/math?formula=z)是隐变量。也就是说模型生成的是单词-话题-文本三元组![(w,z,d)](https://math.jianshu.com/math?formula=(w%2Cz%2Cd))三元组的集合，但观测到的是单词-文本二元组![(w,d)](https://math.jianshu.com/math?formula=(w%2Cd))的集合。观测数据表示为单词-文本矩阵![T](https://math.jianshu.com/math?formula=T)的形式，![T](https://math.jianshu.com/math?formula=T)的行表示单词，列表示文本，元素表示单词-文本对![(w,d)](https://math.jianshu.com/math?formula=(w%2Cd))出现的次数。

![PLSA](../img/ML/PLSA1.png)

从数据生成过程可以推出，文本-单词共现数据![T](https://math.jianshu.com/math?formula=T)出现的概率为：


![P(T)=\prod_{(w,d)}P(w,d)^{n(w,d)}](https://math.jianshu.com/math?formula=P(T)%3D%5Cprod_%7B(w%2Cd)%7DP(w%2Cd)%5E%7Bn(w%2Cd)%7D)

![n(w,d)](https://math.jianshu.com/math?formula=n(w%2Cd))表示![(w,d)](https://math.jianshu.com/math?formula=(w%2Cd))出现的次数，每个单词-文本对生成的概率如下：

![\begin{aligned} P(w,d)&=P(d)P(w|d)\\ &=P(d)\sum_z P(w,z|d)\\ &=P(d)\sum_z P(z|d)P(w|z) \end{aligned}](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20P(w%2Cd)%26%3DP(d)P(w%7Cd)%5C%5C%20%26%3DP(d)%5Csum_z%20P(w%2Cz%7Cd)%5C%5C%20%26%3DP(d)%5Csum_z%20P(z%7Cd)P(w%7Cz)%20%5Cend%7Baligned%7D)

最后一个等号基于在话题![z](https://math.jianshu.com/math?formula=z)给定条件下单词![w](https://math.jianshu.com/math?formula=w)与文本![d](https://math.jianshu.com/math?formula=d)条件独立的假设：

![P(w,z|d)=P(z|d)P(w|z)](https://math.jianshu.com/math?formula=P(w%2Cz%7Cd)%3DP(z%7Cd)P(w%7Cz))

# 共现模型

与生成模型一样，文本-单词共现数据![T](https://math.jianshu.com/math?formula=T)出现的概率为：

![P(T)=\prod_{(w,d)}P(w,d)^{n(w,d)}](https://math.jianshu.com/math?formula=P(T)%3D%5Cprod_%7B(w%2Cd)%7DP(w%2Cd)%5E%7Bn(w%2Cd)%7D)

每个单词-文本对生成的概率如下：

![P(w,d)=\sum_{z\in Z}P(z)P(w|z)P(d|z)](https://math.jianshu.com/math?formula=P(w%2Cd)%3D%5Csum_%7Bz%5Cin%20Z%7DP(z)P(w%7Cz)P(d%7Cz))

共现模型同样假设在话题![z](https://math.jianshu.com/math?formula=z)给定条件下单词![w](https://math.jianshu.com/math?formula=w)与文本![d](https://math.jianshu.com/math?formula=d)条件独立：

![P(w,d|z)=P(w|z)P(d|z)](https://math.jianshu.com/math?formula=P(w%2Cd%7Cz)%3DP(w%7Cz)P(d%7Cz))

其直观图示如下：

![PLSA2](../img/ML/PLSA2.png)

容易验证**生成模型和共现模型是等价的。**但两者性质不同，生成模型中单词变量![w](https://math.jianshu.com/math?formula=w)和文本变量![d](https://math.jianshu.com/math?formula=d)是不对称的，而共现模型中单词变量![w](https://math.jianshu.com/math?formula=w)和文本变量![d](https://math.jianshu.com/math?formula=d)是对称的。因此两个模型的学习算法形式也有所不同。

# PLSA参数估计的EM算法

设单词集合为![W=\{w_1,w_2,\dots,w_M\}](https://math.jianshu.com/math?formula=W%3D%5C%7Bw_1%2Cw_2%2C%5Cdots%2Cw_M%5C%7D)，文本集合为![D=\{d_1,d_2,\dots,d_N\}](https://math.jianshu.com/math?formula=D%3D%5C%7Bd_1%2Cd_2%2C%5Cdots%2Cd_N%5C%7D)，话题集合为![Z=\{z_1,z_2,\dots,z_K\}](https://math.jianshu.com/math?formula=Z%3D%5C%7Bz_1%2Cz_2%2C%5Cdots%2Cz_K%5C%7D)，给定单词-文本共现数据![T=\{n(w_i,d_j)\}](https://math.jianshu.com/math?formula=T%3D%5C%7Bn(w_i%2Cd_j)%5C%7D)，![i=1,2,\dots,M](https://math.jianshu.com/math?formula=i%3D1%2C2%2C%5Cdots%2CM)，![j=1,2,\dots,N](https://math.jianshu.com/math?formula=j%3D1%2C2%2C%5Cdots%2CN)，目标是估计PLSA生成模型的参数。使用极大似然估计，对数似然函数为：

![\begin{aligned} L&=\sum_{i=1}^M\sum_{j=1}^N n(w_i,d_j)\log P(w_i,d_j)\\ &=\sum_{i=1}^M\sum_{j=1}^N n(w_i,d_j)\log[\sum_{k=1}^K P(w_i|z_k)P(z_k|d_j)]\\ \end{aligned}](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20L%26%3D%5Csum_%7Bi%3D1%7D%5EM%5Csum_%7Bj%3D1%7D%5EN%20n(w_i%2Cd_j)%5Clog%20P(w_i%2Cd_j)%5C%5C%20%26%3D%5Csum_%7Bi%3D1%7D%5EM%5Csum_%7Bj%3D1%7D%5EN%20n(w_i%2Cd_j)%5Clog%5B%5Csum_%7Bk%3D1%7D%5EK%20P(w_i%7Cz_k)P(z_k%7Cd_j)%5D%5C%5C%20%5Cend%7Baligned%7D)

接下来通过EM算法迭代学习模型的参数即可，最终得到![P(w_i|z_k)](https://math.jianshu.com/math?formula=P(w_i%7Cz_k))和![P(z_k|d_j)](https://math.jianshu.com/math?formula=P(z_k%7Cd_j))。