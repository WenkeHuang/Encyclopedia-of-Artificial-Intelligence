# 主成分分析

关键词：基于最小最大投影距离，算法流程

主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。一般我们提到降维最容易想到的算法就是PCA。

# PCA的思想

PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是$n$维的，共有m个数据$(𝑥_{(1)},𝑥_{(2)},...,𝑥_{(𝑚)})$。我们希望将这$m$个数据的维度从$n$维降到$n'$维，希望这$m$个$n'$维的数据集尽可能的代表原始数据集。我们知道数据从$n$维降到$n'$维肯定会有损失，但是我们希望损失尽可能的小。

- 样本点到这个直线的距离足够近
- 样本点在这个直线上的投影能尽可能的分开。

# PCA的推导:基于最小投影距离

假设m个n维数据$(x^{(1)}, x^{(2)},...,x^{(m)})$都已经进行了中心化，即$\sum\limits_{i=1}^{m}x^{(i)}=0$。经过投影变换后得到的新坐标系为$\{w_1,w_2,...,w_n\}$其中$w$是标准正交基，即$||w||_2=1, w_i^Tw_j=0$

如果我们将数据从n维降到n'维，即丢弃新坐标系中的部分坐标，则新的坐标系为$\{w_1,w_2,...,w_{n'}\}$，样本点$x^{(i)}$在n'维坐标系中的投影为：$z^{(i)} = (z_1^{(i)}, z_2^{(i)},...,z_{n'}^{(i)})^T$，$z_j^{(i)} = w_j^Tx^{(i)}$是$x^{(i)}$在低维坐标系里第j维的坐标。

如果我们用$z^{(i)}$，来恢复原始数据$x^{(i)}$，则得到的恢复数据$\overline{x}^{(i)} = \sum\limits_{j=1}^{n'}z_j^{(i)}w_j = Wz^{(i)}$其中，W为标准正交基组成的矩阵。

**现在我们考虑整个样本集，我们希望所有的样本到这个超平面的距离足够近，即最小化下式：**
$$
\sum\limits_{i=1}^{m}||\overline{x}^{(i)} - x^{(i)}||_2^2
$$
将这个式子进行整理，可以得到:
$$
\begin{align} \sum\limits_{i=1}^{m}||\overline{x}^{(i)} - x^{(i)}||_2^2 & = \sum\limits_{i=1}^{m}|| Wz^{(i)} - x^{(i)}||_2^2 \\& = \sum\limits_{i=1}^{m}(Wz^{(i)})^T(Wz^{(i)}) - 2\sum\limits_{i=1}^{m}(Wz^{(i)})^Tx^{(i)} + \sum\limits_{i=1}^{m} x^{(i)T}x^{(i)} \\& = \sum\limits_{i=1}^{m}z^{(i)T}z^{(i)} - 2\sum\limits_{i=1}^{m}z^{(i)T}W^Tx^{(i)} +\sum\limits_{i=1}^{m} x^{(i)T}x^{(i)} \\& = \sum\limits_{i=1}^{m}z^{(i)T}z^{(i)} - 2\sum\limits_{i=1}^{m}z^{(i)T}z^{(i)}+\sum\limits_{i=1}^{m} x^{(i)T}x^{(i)}  \\& = - \sum\limits_{i=1}^{m}z^{(i)T}z^{(i)} + \sum\limits_{i=1}^{m} x^{(i)T}x^{(i)}  \\& =   -tr( W^T（\sum\limits_{i=1}^{m}x^{(i)}x^{(i)T})W)  + \sum\limits_{i=1}^{m} x^{(i)T}x^{(i)} \\& =  -tr( W^TXX^TW)  + \sum\limits_{i=1}^{m} x^{(i)T}x^{(i)}  \end{align}
$$
注意到$\sum\limits_{i=1}^{m}x^{(i)}x^{(i)T}$是数据集的协方差矩阵，W的每一个向量$w_j$是标准正交基。而\sum\limits_{i=1}^{m} x^{(i)T}x^{(i)}是一个常量。最小化上式等价于：
$$
\underbrace{arg\;min}_{W}\;-tr( W^TXX^TW) \;\;s.t. W^TW=I
$$
这个最小化不难，直接观察也可以发现最小值对应的$W$由协方差矩阵$XX^T$最大的n'个特征值对应的特征向量组成。当然用数学推导也很容易。利用拉格朗日函数可以得到：
$$
J(W) = -tr( W^TXX^TW + \lambda(W^TW-I))
$$
对W求导有：
$$
XX^TW=\lambda W
$$

# PCA的推导:基于最大投影方差

 对于任意一个样本$x^{(i)}$，在新的坐标系中的投影为$W^Tx^{(i)}$，在新坐标系中的投影方差为$x^{(i)T}WW^Tx^{(i)}$，要使所有的样本的投影方差和最大，也就是最大化$\sum\limits_{i=1}^{m}W^Tx^{(i)}x^{(i)T}W$的迹,即：
$$
\underbrace{arg\;max}_{W}\;tr( W^TXX^TW) \;\;s.t. W^TW=I
$$
观察第二节的基于最小投影距离的优化目标，可以发现完全一样，只是一个是加负号的最小化，一个是最大化。

利用拉格朗日函数可以得到
$$
J(W) = tr( W^TXX^TW + \lambda(W^TW-I))
$$
对W求导有，整理：
$$
XX^TW=（-\lambda）W
$$

# 算法流程

假定我们需要将特征维度从 n维降到k维，则PCA的执行流程

**特征标准化**，平衡各个特征尺度：
$$
x_j^{(i)}=\frac{x_j^{(i)}-\mu_j}{s_j}, \mbox{$\mu_j$ 为特征 $j$ 的均值，$s_j$ 为特征 $j$ 的标准差。}
$$
计算**协方差矩阵** :
$$
\Sigma = \frac{1}{m}\sum\limits_{i=1}^{m}(x^{(i)})(x^{(i)})^T = \frac{1}{m} \cdot X^TX
$$
通过**奇异值分解（SVD）**，求取特征向量（eigenvectors）：
$$
(U,S,V^T) = SVD(\Sigma)
$$
从 U 中取出前 k个左奇异向量，构成一个约减矩阵 Ureduce:
$$
U_{reduce} = (u^{(1)},u^{(2)},\cdots,u^{(k)})
$$
计算新的特征向量：
$$
z^{(i)}=U_{reduce}^T \cdot x^{(i)}
$$

# PCA算法总结

这里对PCA算法做一个总结。作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如第六节的为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。

PCA算法的主要优点有：

1）仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　

2）各主成分之间正交，可消除原始数据成分间的相互影响的因素。

3）计算方法简单，主要运算是特征值分解，易于实现。

PCA算法的主要缺点有：

1）主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。

2）方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

