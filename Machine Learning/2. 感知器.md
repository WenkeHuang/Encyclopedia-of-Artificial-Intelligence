# 感知器

关键词：感知器模型，感知器线性可分，原始形式，收敛性，对偶性

# 概述

  感知机学习旨在求出将训练数据集进行线性划分的分类超平面（线性不可分的训练样例不能被感知器学习）。感知机模型是神经网络和支持向量机的基础。下面分别从感知机学习的模型、策略和算法三个方面来介绍。

# 感知器模型



输入空间是$\mathcal{X}\subseteq R^n$，输出空间是$y=\{+1,-1\}$，输入$x \in  \mathcal{X} $表示实例的特征向量，对应于输入空间的点，输出$y \in \mathcal(Y)$表示实例的类别。
$$
f(x)=sign(w\cdot x + b)
$$

$$
sign(x)=\begin {cases}
+1, & x\geq0 \\\
-1, & x<0
\end {cases}
$$

对应于特征空间$R^n$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。超平面$S$称为分离超平面（separating hyperplane）

# 感知器学习策略

## 数据集的线性可分性

$$
T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}
$$

其中：
$$
x_i \in \mathcal{X}=R^n,y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...N
$$
如果存在某个超平面$S$：
$$
w \cdot x + b = 0 
$$
能够将正实例点和负实例点完全正确地划分到两侧，则$T$数据集为线性可分数据集（linearly separable data set）

## 感知器学习策略

输入空间$R^n$中国任一点$x_0$到超平面$S$的距离：
$$
\frac{1}{||w||}|w \cdot x_0+b|,||w||表示w的L_2范式
$$
定义$M$为误分类点的集合，这个损失函数就是感知机学习的经验风险函数。
$$
L(w,b)= -\sum_{x_i \in M}y_i(w \cdot x_i+b)
$$

# 感知器学习算法

## 感知机学习算法的原始形式

$$
T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}
$$

其中
$$
x_i \in \mathcal{X}=R^n,y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...N
$$
求参数$w,b$，使其为损失函数极小化问题的解
$$
\min_{w,b}=-\sum_{x_i \in M}y_i(w \cdot x_i +b)
$$




$$
\nabla_w L(w,b)=-\sum_{x_i \in M}y_ix_i
$$




$$
\nabla_b L(w,b)=-\sum_{x_i \in M}y_i
$$


随机选出一个误分类点$(x_i,y_i)$，对$w，b$进行更新。


$$
w\leftarrow w + \eta y_i x_i
$$




$$
b\leftarrow b+\eta y_i
$$

## 算法的收敛性



为了便于叙述与推倒，将偏置$b$并入权重向量$w$，记作：
$$
\hat{w}=(w^T,b^T)
$$
同样也将输入向量加入以扩充，加进常数1，记作：
$$
\hat{x}=(x^T,1)^T
$$
得到：
$$
\hat{w}\cdot \hat{x}=w \cdot x + b
$$
**Novikoff：**

设训练数据集线性可分：
$$
T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}
$$

$$
y_i \in \mathcal{Y}=\{-1,+1\},
$$

$$
x_i \in \mathcal{X}=R^n
$$

$$
i=1,2,....N
$$

Step1：


$$
\exists	y_i(\hat{w}_{opt} \cdot \hat{x}_i) = y_i(w_{opt}\cdot + b_{opt}) \geq	\gamma
$$



Step2：
$$
Let R = \max_{1\leq i\leq N}||\hat{x_i}||
$$



$$
误分类次数k满足 k \leq(\frac{R}{\gamma})^2
$$

## 感知机学习算法的对偶形式

当训练数据集线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或迭代顺序而可能有所不同。

































