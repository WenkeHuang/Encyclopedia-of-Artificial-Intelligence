# 统计学习方法

方法=模型+策略+算法

监督学习, 非监督学习, 强化学习都有这样的三要素.

这里回顾一下第一章的统计学习三要素:

1. **模型**
   1. 监督学习中, 模型就是所要学习的条件概率分布或者决策函数.
2. **策略**
   1. 统计学习的目标在于从假设空间中选取最优模型.
   2. 损失函数度量一次预测的好坏; **风险函数**度量平均意义下模型预测的好坏.
   3. 经验风险最小化(ERM)与结构风险最小化(SRM)
   4. 经验风险或者结构风险是最优化的目标函数.
3. **算法**
   1. 统计学习基于训练数据集, 根据学习策略, 从假设空间中选择最优模型, 最后需要考虑用干什么样的计算方法求解**最优模型**.
   2. 统计学习问题转化为最优化问题.
      1. 有显式解析解, 对应的最优化问题比较简单
      2. 通常解析解不存在, 需要通过数值计算的方式求解.
   3. 算法需要解决的问题是如何找到**全局最优解**, 并且求解的过程非常高效

# 不同视角

这本书的内容可以从多个角度进行划分

1. 简单分类方法
   1. 感知机
   2. k近邻法
   3. 朴素贝叶斯法
   4. 决策树
2. 复杂分类方法
   1. 逻辑斯谛回归模型
   2. 最大熵
   3. 支持向量机
   4. 提升方法
3. 标注方法
   1. 隐马尔科夫模型
   2. 条件随机场

# 模型

分类问题与标注问题都可以认为是从输入空间到输出空间的映射。

**他们可以写成条件概率分布$P(Y|X)$或者决策函数$Y=f(x)$的形式。**

## 概率模型和非概率模型

对应**概率模型**和**非概率模型**.

1. 概率模型(由条件概率表示的模型)
   1. 朴素贝叶斯
   1. 隐马尔科夫模型
1. 非概率模型(由决策函数表示的模型)
   1. 感知机
   1. k近邻
   1. 支持向量机
   1. 提升方法
1. 概率模型和非概率模型
   1. 决策树
   1. 逻辑斯谛回归模型
   1. 最大熵模型
   1. 条件随机场

## 生成模型和判别模型

1. 判别模型
   1. 直接学习条件概率分布$P(Y|X)$或者决策函数$Y=f(X)$的方法为判别方法，对应的模型为判别模型。
   1. 感知机，k近邻，决策树，逻辑斯谛回归模型，最大熵模型，支持向量机，提升方法，条件随机场
1. 生成模型
   1. 先学习联合概率分布$P(X, Y)$，从而求得条件概率分布$P(Y|X)$的方法是生成方法，对应的模型是生成模型。
   1. 朴素贝叶斯，隐马尔科夫模型

## 线性模型和非线性模型

1. 线性模型
   1. 感知机
1. 对数线性模型
   1. 逻辑斯谛回归模型
   1. 最大熵模型
   1. 条件随机场
1. 非线性模型
   1. k近邻
   1. 决策树
   1. 支持向量机(核函数)
   1. 提升方法

## 生成与判别, 分类与标注

|      | 判别   | 生成 |
| ---- | ------ | ---- |
| 分类 | LR, ME | NB   |
| 标注 | CRF    | HMM  |

# 学习策略

## 损失函数

注意，书中这里描述的是，**在二分类的监督问题中**，后面会在这个基础上做推广。

1. 合页损失

   线性支持向量机

   $max(0, 1- yf(x)) $

1. 逻辑斯谛损失函数

   逻辑斯谛回归模型与最大熵模型

   $log(1+exp(-yf(x)))$

1. 指数损失函数

   提升方法

   $exp(- yf(x))$

三种损失函数都是0-1损失函数的上界. 

![Loss](../img/ML/LOSS.png)

上面这个图有几点要注意:

1. logistic loss，里面的对数是2
1. 另外，这些函数在0右侧的部分，都是有值的。
1. 分类问题的损失，实现二分类任务

这几个模型，用在分类问题上，可以有一种统一的表达来描述损失函数。这会引入**经验风险最小化**和**结构风险最小化**。

学习的策略是优化以下结构风险函数

$$ \min \limits_{f \in {H}} \frac {1}{N} \sum \limits^{N}_{i=1} L(y_i , f(x_i)) + \lambda J(f)$$

第一项为经验风险(经验损失)，第二项为正则化项

# 统计学习方法总结

| 方法                     | 适用问题         | 模型特点                                           | 模型类型 | 学习策略                           | 学习的损失函数       | 学习算法                               |
| ------------------------ | ---------------- | -------------------------------------------------- | -------- | ---------------------------------- | -------------------- | -------------------------------------- |
| 感知机                   | 二类分类         | 分离超平面                                         | 判别     | 极小化误分点到超平面距离           | 误分点到超平面的距离 | 随机梯度下降                           |
| k近邻法                  | 多类分类，回归   | 特征空间、样本点                                   | 判别     |                                    |                      |                                        |
| 朴素贝叶斯法             | 多类分类         | 特征与类别的联合概率分布，条件独立假设             | 生成     | 极大似然估计，极大后验概率估计     | 对数似然损失         | 概率计算公式、EM算法                   |
| 决策树                   | 多类分类，回归   | 分类书，回归树                                     | 判别     | 正则化的极大似然估计               | 对数似然损失         | 特征选择、生成、剪枝                   |
| 逻辑斯谛回归与最大熵模型 | 多类分类         | 特征条件下类别的条件概率分布，对数线形模型         | 判别     | 极大似然估计，正则化的极大似然估计 | 逻辑斯谛损失         | 改进的迭代尺度算法、梯度下降、拟牛顿法 |
| 支持向量机               | 二类分类         | 分离超平面，核技巧                                 | 判别     | 极小化正则化合页损失，软间隔最大化 | 合页损失             | 序列最小最优化算法                     |
| 提升方法                 | 二类分类         | 弱分类器的线性组合                                 | 判别     | 极小化加法模型的指数损失           | 指数损失             | 前向分步加法模型                       |
| EM算法                   | 概率模型参数估计 | 含隐变量概率模型                                   |          | 极大似然估计，极大后验概率估计     | 对数似然估计         | 迭代算法                               |
| 隐马尔可夫模型           | 标注             | 观测序列与状态序列的联合概率分布模型               | 生成     | 极大似然估计，极大后验概率估计     | 对数似然估计         | 概率计算公式、EM算法                   |
| 条件随机场               | 标注             | 状态序列条件下观测序列的条件概率分布，对数线性模型 | 判别     | 极大似然估计，正则化极大似然估计   | 对数似然估计         | 改进的迭代尺度算法、梯度下降、拟牛顿法 |



































