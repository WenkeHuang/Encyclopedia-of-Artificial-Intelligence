# k近邻算法

关键词：距离度量，k值选择，kd树

Step1：
$$
T = {(x_1,y_1),(x_2,y_2),...(x_N,y_N)}
$$



$$
x_i \in \mathcal{X} \subseteq R^n 
$$

$$
y_i \in \mathcal{Y}=\{c_1,c_2,...c_k\}  i = 1,2,..N
$$


Step2：

根据给定的距离度量，在训练集T重找出与$x$最邻近的$k$个点，涵盖着$k$个点的$x$的领域记作$N_k(x)$
$$
y = arg \max_{c_j} \sum_{x_i \in N_k(x)}I(y_i = c_j),i=1,2,...N
$$

# k近邻模型

## 模型

**k近邻模型**实质上是一个空间划分模型。根据训练样本自身的特征，通过距离公式计算，将训练数据集组成空间整体划分成M个字空间（M为类别数）。利用测试集进行测试评估模型的好快，以调整k的选择或者距离方法的选择。在此，经常使用交叉验证的方法。

## 距离度量

**特征空间**中两个实例点的距离是两个实例点相似程度的反映。在此我们介绍一些我们经常用到的一些距离公式：

Assumption：
$$
设特征空间\mathcal{X}是n维实数向量空间R^n，x_i,x_j \in \mathcal{X},x_i=(x_i^{(1)},x_i^{(2)},..x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},..x_j^{(n)})^T,x_i,x_j的L_p距离定义为：
$$

$$
L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}
$$




其中当$p=2$，称为欧式距离(Euclidean distance)：
$$
L_2(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}
$$
其中当$p=1$，称为曼哈顿距离(Manhattan distance)：
$$
L_2(x_i,x_j)=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|
$$
其中当$p=\infty$，称为曼哈顿距离(Manhattan distance)：
$$
L_{\infty}(x_i,x_j) = \max_l|x_i^{(l)}-x_j^{(l)}|
$$

## k值的选择

k值的选择会对k近邻算法的结果产生重大影响。

k值的增大意味着模型变得简单。如果k=N，那么无论输入什么，都将简单的预测它属于在训练实例中最多的类。这个时候，模型过于简单，完全忽略了实例中大量的有用信息，只考虑了类别数量，这是不可取的。

## 分类决策规则

k近邻算法中的分类决策规则往往是多数表决，即由输入实例的$k$个近邻的训练实例中的多数类决定输入实例的类别。

**多数表决规则(majority voting rule)**

假设分类的损失函数为0-1损失函数，误分类的概率是：
$$
P(Y \neq f(X))= 1-P(Y = f(X))
$$
对于给定的实例$x \in \mathcal{X}$，其最近邻的$k$个训练实例点构成集合$N_k()$，如果涵盖$N_k$的区域的类别是$c_j$，那么误分类率是：
$$
\frac{1}{k} \sum_{x_i \in N_k(x)}I(y_i \neq c_j)= 1\frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i=c_j)
$$


# k近邻法的实现：kd树

kd树是一种对k为空间中的实例进行存储以便对齐进行快速检索的属性数据结构。kd树是二叉树，表示对k为空间的一个划分。构造kd树相当于不断地垂直于坐标轴的超平面将k为空间划分。构成一系列的k为超矩形区域。kd树的每个节点对应于一个k维超矩形区域。



构造kd树的方法：
构造根节点，使根节点对应于k维空间中包含所有实例点的超矩形区域；
通过下面的递归方法，不断的对k为空间进行切分，生成子节点。
在超矩形区域（结点）上选择一个坐标轴和此坐标轴上的一个且分点，确定一个超平面，这个超平面通过选定的且分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域，这个过程知道区域内没有实例时终止（终止时的结点是叶节点）。在此过程，将实例保存在响应的结点上。

通常情况下。依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数为且分点，这样得到的kd树是平衡的。（平衡的kd树搜索效率未必最优）







































