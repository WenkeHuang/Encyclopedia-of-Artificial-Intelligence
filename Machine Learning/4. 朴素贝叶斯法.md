# 朴素贝叶斯法

关键词：后验概率，极大似然估计，学习与分类，贝叶斯估计

# 相关的统计学知识

贝叶斯学派的思想可以概括为**先验概率+数据=后验概率**。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。

条件独立公式，如果$X$和$Y$相互独立，则有：
$$
P(X,Y)=P(X)P(Y)
$$
条件概率公式：
$$
P(Y|X)=P(X,Y)/P(X)
$$

$$
P(X|Y)=P(X,Y)/P(Y)
$$
全概率公式：
$$
P(X)= \sum _k P(X|Y=Y_k)P(Y_k)
$$

$$
\sum_kP(Y_k)=1
$$
贝叶斯公式：
$$
P(Y_k|X)=\frac{P(X|Y_k)P(Y_k)}{\sum_kP(X|Y=Y_k)P(Y_k)}
$$

# 朴素贝叶斯法的学习与分类

## 基本方法

输入空间$\mathcal{x}\subseteq R^n$，输出空间$\mathcal{Y}={c_1,c_2,...,c_k}$，输入为特征向量$x\in \mathcal{X}$，输出为类标记$y\in\mathcal{Y}$，
$$
y\in\mathcal{Y}
$$


训练数据集：
$$
T = {(x_1,y_1),(x_2,y_2),....,(x_N,y_N)}
$$
先验概率分布：
$$
P(Y=C_k),k=1,2,...,k
$$
条件概率分布：
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...X^{(n)}=x^{(n)}|Y=c_k),k=1,2,...,k
$$
同样地等价于：
$$
P(X=x|Y=c_k)=\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)
$$
后验概率计算根据贝叶斯定理：
$$
P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}
$$
综合式子得到：
$$
P(Y=c_k|X=x)=\frac{P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)},k=1,2,...K
$$
得到朴素贝叶斯分类起可表示为：
$$
y = f(x)=arg \max_{c_k} \frac{P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}
$$
考虑到分母相同：
$$
y = f(x)=arg \max_{c_k} {P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}
$$

## 后验概率最大化的含义

假设选择0-1损失函数：
$$
L(Y,f(X))=\begin {cases}
1, & Y \neq f(X) \\\
0, & Y = f(X)
\end {cases}
$$
此时，$f(X)$是分类决策函数，期望风险函数：
$$
R_{exp}(f)=E[L(Y,f(X)]
$$
期望是对联合分布P(X,Y)取的：
$$
R_{exp}(f)=E_\mathcal{X} \sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)
$$
这样一来，根据期望风险最小化准则就得到了后验概率最大化准则：
$$
f(x) = arg \max _{y \in \mathcal{Y}} P(c_k|X=x)
$$

# 朴素贝叶斯法的参数估计

## 极大似然估计

先验概率$P(Y=c_k)$的极大似然估计是：
$$
P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N},k=1,2,...K
$$
设第$j$个特征$x^{j}$可能取值的集合为$\{a_{j1},a_{j2},...,a_{jS_j}\}$，条件概率$P(X^{(j)}=a_{ji}|Y=c_k)$极大似然估计是：
$$
P(X^{(j)}=a_{ji}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^{(j)}=a_{ji},y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}
$$

$$
j=1,2...,n; l=1,2,...,S_j;k=1,2,...,K
$$

## 学习与分类算法

**朴素贝叶斯算法(naie Bayes algorithm)**

Step1计算先验概率及条件概率：
$$
p(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,..,K
$$

$$
P(X^{(j)}=a_{ji}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x_i^{(j)}=a_{ji},y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}
$$

$$
j=1,2...,n; l=1,2,...,S_j;k=1,2,...,K
$$
Step2 对于给定的实例$x$：
$$
P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k),k=1,2,...,K
$$
Step3 确定实例x的类
$$
y = arg \max_{c_k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
$$

## 贝叶斯估计

极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。
$$
P_A(X^{j}=a_{ji}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)},y_i=c_k)+\lambda}{\sum_{i=1}^N I(y_i=c_k)+S_j \lambda}
$$


































