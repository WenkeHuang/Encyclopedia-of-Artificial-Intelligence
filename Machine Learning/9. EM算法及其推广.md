# EM算法的引入

我们经常会从样本观察数据中，找出样本的模型参数。 最常用的方法就是极大化模型分布的对数似然函数。

但是在一些情况下，我们得到的观察数据**有未观察到的隐含数据**，此时我们未知的有隐含数据和模型参数，因而无法直接用极大化对数似然函数得到模型分布的参数。怎么办呢？这就是EM算法可以派上用场的地方了。

EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以**先猜想隐含数据（EM算法的E步）**，接着基于观察数据和猜测的隐含数据一起来极大化对数似然，**求解我们的模型参数（EM算法的M步)**。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。

从上面的描述可以看出，EM算法是迭代求解最大值的算法，同时算法在每一次迭代时分为两步，E步和M步。**一轮轮迭代更新隐含数据和模型分布参数，直到收敛**，即得到我们需要的模型参数。

一个最直观了解EM算法思路的是K-Means聚类算法原理。在K-Means聚类时，每个聚类簇的质心是隐含数据。我们会**假设𝐾个初始化质心**，即EM算法的E步；**然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即EM算法的M步**。重复这个E步和M步，直到质心不再变化为止，这样就完成了K-Means聚类。

当然，K-Means算法是比较简单的，实际中的问题往往没有这么简单。上面对EM算法的描述还很粗糙，我们需要用数学的语言精准描述。

## 三硬币分析

假设有三枚硬币，分别记为A、B、C。这些硬币正面的概率分别为π，p，q，进行如下的抛硬币实验：先掷硬币A，根据其结果选出硬币B或者硬币C，正面选硬币B，反面选硬币C，然后掷选出的硬币，掷硬币的记过，出现正面记作1，出现反面记作0，独立地重复n次实验（这里n=10），然后观测结果如下：

1，1，0，1，0，0，1，0，1，1

假设只能观测到掷硬币的结果，不能观测掷硬币的过程，问如何估计三硬币正面出现的概率，即三硬币模型的参数π，p，q。

三硬币模型可以写作：
$$
P(y|\theta)=\sum_zP(y,z|\theta)=\sum_zP(z|\theta)P(y|z,\theta)\\
=\pi p^y(1-p)^{1-y}+(1-\pi)q^y(1-q)^{1-y}
$$
将观测数据表示为：$Y = (Y_1,Y_2,...,Y_n)^T$，未观测数据表示为$Z = (Z_1,Z_2,...,Z_n)^T$，则观测数据的似然函数为：
$$
P(Y|\theta)=\sum_zP(Z|\theta)P(Y|Z,\theta)
$$
即：
$$
P(Y|\theta)=\prod_{j=1}^n[\pi p^{y_j}(1-p)^{1-y_j}+(1-\pi)q^{y_j}(1-q)^{1-y_j}]
$$
考虑求模型参数$\theta =(\pi，p，q)$的极大似然估计
$$
\widehat{\theta}=arg\max_{\theta}log(P(Y|\theta))
$$
**步骤分析**

Step1初始化：
$$
\theta^{(0)}=(\pi^{(0)},p^{(0)},q^{(0)})
$$
Step2第i次迭代结果：
$$
\theta^{(i)}=(\pi^{(i)},p^{(i)},q^{(i)})
$$
Step3E步-计算在实时参数下观测到数据$y_j$来自掷硬币B的概率：
$$
\mu^{(i+1)}=\frac{\pi^{(i)}(p^{(i)})^{y_j}(1-p^{(i)})^{1-y_j}}{\pi^{(i)}(p^{(i)})^{y_j}(1-p^{(i)})^{1-y_j}+(1-\pi^{(i)})(q^{(i)})^{y_j}(1-q^{(i)})^{1-y_j}}
$$
Step4M步，计算模型参数的新估计值：
$$
\pi^{(i+1)}=\frac{1}{n}\sum_{j=1}^n\mu_j^{i+1} \\
p^{(i+1)}\frac{\sum_{j=1}^n\mu_j^{(i+1)}y_j}{\sum_{j=1}^n\mu_j^{(i+1)}}\\
q^{(i+1)}=\frac{\sum_{j=1}^n（1-\mu_j^{(i+1)}）y_j}{\sum_{j=1}^n（1-\mu_j^{(i+1)}）}
$$

## EM算法

输入：观测变量数据



































