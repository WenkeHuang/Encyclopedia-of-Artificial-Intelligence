# EM算法的引入

我们经常会从样本观察数据中，找出样本的模型参数。 最常用的方法就是极大化模型分布的对数似然函数。

但是在一些情况下，我们得到的观察数据**有未观察到的隐含数据**，此时我们未知的有隐含数据和模型参数，因而无法直接用极大化对数似然函数得到模型分布的参数。怎么办呢？这就是EM算法可以派上用场的地方了。

EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以**先猜想隐含数据（EM算法的E步）**，接着基于观察数据和猜测的隐含数据一起来极大化对数似然，**求解我们的模型参数（EM算法的M步)**。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。

从上面的描述可以看出，EM算法是迭代求解最大值的算法，同时算法在每一次迭代时分为两步，E步和M步。**一轮轮迭代更新隐含数据和模型分布参数，直到收敛**，即得到我们需要的模型参数。

一个最直观了解EM算法思路的是K-Means聚类算法原理。在K-Means聚类时，每个聚类簇的质心是隐含数据。我们会**假设𝐾个初始化质心**，即EM算法的E步；**然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即EM算法的M步**。重复这个E步和M步，直到质心不再变化为止，这样就完成了K-Means聚类。

当然，K-Means算法是比较简单的，实际中的问题往往没有这么简单。上面对EM算法的描述还很粗糙，我们需要用数学的语言精准描述。

## 

































