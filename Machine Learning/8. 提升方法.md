# 提升方法

关键词：AdaBoost算法，前向分步，提升树

# 提升方法AdaBoost算法

## 提升方法的基本思路

对于一个复杂任务来说， 将多个专 家的判断进行适当的综合所得出的判断， 要比其中任何一个专家单独 的判断好。 实际上， 就是“三个臭皮匠顶个诸葛亮”的道理  

 在概率近似正确（probably approximately correct， PAC） 学习的框架中， 如果存在一个多项式的学习算法能够学习它， 并 且正确率很高， 那么就称这个概念是强可学习的； 

如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学的。

**提升方法的核心问题**

- 在每一轮如何改变训练数据的权值或概率分布

  通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。。

- 如何将弱分类器组合成一个强分类器

  通过加法模型将弱分类器进行线性组合，比如 AdaBoost 通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。。
  

## Adaboost

**Idea**

- AdaBoost的做法是**提高那些被前一轮弱分类器错误分类样本的权值， 而降低那些被正确分类样本的权值**。 这样一来， 那些没有得到正确分类的数据， 由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。 
- 即弱分类器的组合， AdaBoost采取加权多数表决的方法。 具体地， 加大分类误差率小的弱分类器的权值， 使其在表决中起较大的作用， 减小分类误差率大的弱分类器的权值， 使其在表决中起较小的作用。AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一 种算法里。  

**Step**

输入：
$$
T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}，x_i\in\mathcal{X}\subseteq
 R^n，y_i \in\mathcal{Y}=\{-1,+1\}
$$
初始化权值分布：
$$
D_1=(w_{11},...w_{1i},...,w_{1N})，w_{1i}=\frac{1}{N}，i=1,2,...,N
$$
基本分类器：
$$
G_m(x): \mathcal{X}\to\{-1,+1\}
$$
分类误差率：
$$
e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)
$$
系数：
$$
\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}
$$
更新权值分布：
$$
D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})
$$
以及：
$$
w_{m+1,i} = \frac{w_{m,i}}{Z_m}exp(-\alpha_my_iG_m(x_i)),i=1,2,...,N
$$
这里的$Z_m$是规范化因子：
$$
Z_m = \sum_{i=1}^Nw_{m,i}exp(-\alpha_my_iG_m(x))
$$
最终分类器：
$$
G(x)=sign(\sum_{m=1}^M\alpha_mG_m(x))
$$


# AdaBoost算法的训练误差分析

AdaBoost最基本的性质是它能在学习过程中不断减少训练误差。
$$
\frac{1}{N}\sum_{i=1}^NI(G(x_i)\neq y_i)\leq\frac{1}{N}\sum_iexp(-y_if(x_i))=\prod
_mZ_m
$$




# AdaBoost算法的解释

## 前向分步算法

**加法模型**
$$
f(x)=\sum_{m=1}^m\beta_mb(x;\gamma_m)
$$
**损失函数极小化**
$$
\min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i,\sum_{m=1}^M\beta_mb(x_i;\gamma_m))
$$
具体地，每步只需优化如下损失函数：
$$
\min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i,\beta b(x_i;\gamma))
$$

## 前向分步算法与AdaBoost

可认为AdaBoost是模型为加法模型，损失函数为指数函数，学习算法为前向分步算法时的二分类学习方法。

# 提升树

提升树被认为是统计学习中性能最好的方法之一。

提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（boosting tree）。

提升树是以分类树或回归树为基分类器。它的idea在于，第一个回归树预测的效果可能一般，但是第二个回归树把第一个预测错的残差作为输入。**也就是说，如果一个点的值被预测错误，那么在下一个回归树里面的模型的权值会变大。**通过这个方式，来提高模型的效果。

**提升树模型**
$$
f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
$$
第m步的模型是：
$$
f_m(x)=f_{m-1}(x)+T(x;\Theta_m)
$$
经验风险极小化:
$$
\widehat{\Theta_m}=arg\min_{\Theta_m}=\sum_{i=1}^NL(y_i,f_{m-1}(x)+T(x;\Theta_m))
$$






































