# 统计学习
## 统计学习的特点
1. 统计学以计算机及网络为平台，是建立在计算机及网络之上的
2. 统计学习以数据为研究对象，是数据驱动的学科
3. 统计学习的目的是对数据进行预测与分析
4. 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析
5. 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系与方法论.
## 统计学习的对象
统计学习的对象是数据。它从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提。这里的同类数据是指具有某种共同性质的数据。
## 统计学习的方法
统计学习的方法是基于数据构建统计模型从而对数据进行预测与分析，统计学习由监督学习、非监督学习、半监督学习和强化学习等组成。  
我们主要讨论监督学习，这种情况下统计学习的方法可以概括如下：从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间；应用某个评价准则，从假设空间种选取一个最优的模型，使它对已知的数据及未知的测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。这样，统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法，称其为统计学习方法的三要素，模型、策略和算法
# 监督学习 supervised learning
## 基本概念
- 输入空间、特征空间、输出空间
在监督学习中，将输入与输出所有可能取值的集合分别称为输入空间（input space）与输出空间（output space）。
每个具体的输入是一个实例（instance），通常由特征向量（feature vector）表示，所有的特征向量存在的空间称为特征空间（feature space）。
- 联合概率分布
监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)。P(X,Y)表示分布函数，或分布密度函数。
- 假设空间
监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。
换句话说，学习的目的就在于找到最好的这样的模型。
模型属于由输入空间到输出空间的映射的集合。这个集合就是假设空间（hypothesis space）。
# 统计学习的三要素
## 模型
模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。
假设空间可以用$\mathcal{F}$表示，可以定义为决策函数的集合,称为非概率模型。  
$$
\mathcal{F} = \{f|Y=f(X)\}
$$
其中，$X$和$Y$是定义在输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$的变量，这时$\mathcal{F}$通常是由一个参数向量决定的函数族：
$$
\mathcal{F} = \{f|Y=f_\theta(X),\theta \in R^2\}
$$
参数向量$\theta$取值于n维欧氏空间$R^n$，称为参数空间(parameter space)
## 策略
1. 损失函数和风险函数


监督学习问题是在假设空间下中选取模型$\mathcal{F}$作为决策函数，对于给定的输入$X$,由$f(X)$给出相应的输出$Y$，这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，用一个损失函数（loss function）或代价函数（cost function）来度量 预测错误的程度．损失函数是$f(X)$和$Y$的非负实值函数，记作$L(Y,f(X))$. 统计学习常用的损失函数有以下几种： 
- 0-1损失函数(0-1 loss function)
  
  $$
  L(Y,f(x))=\begin{cases}
  1 & Y \neq f(X) \\
  0 & Y = f(X)
  \end{cases}
  $$


- 绝对损失函数(absolute loss function)

$$
L(Y,f(X))=|Y-f(x)|
$$



- 平方损失函数(quadratic loss function)

$$
L(Y,f(x))=(Y-f(x))^2
$$


- 对数损失函数(logarithmic loss function)

$$
L(Y|P(Y|X)=-logP(Y|X)
$$

损失函数值越小，模型就越好，由于模型的输入、输出 $(X,Y)$ 是随机变量，遵循联合分布$P(X,Y)$，所以损失函数的期望是	
$$
R_{exp}(f)=E~p~[L(Y,f(X))]	=\int_{\mathcal{x}\times\mathcal{y}} ~L(y,f(x))P(x,y)dxdy
$$
这是理论上模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数(risk function)或期望损失(expected loss)	

模型$f(X)$关于训练数据集的平均损失称为经验风险(empirical risk)或经验损失(empirical loss)，记为$R_{emp}$:
$$
R_{emp}(f)=\frac{1}{N}\sum_i^NL(y_i,f(x_i))
$$


> 期望风险$R_{exp}(f)$是模型关于联合分布的期望损失，经验风险$R_{emp}(f)$是模型关于训练样本集的平均损失。



2. 经验风险最小化与结构风险最小化

经验风险最小化（empirical risk minimization,ERM）的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：
$$
\min_{f \in \mathcal(F)}\frac{1}{N}\sum_i^NL(y_i,f(x_i))
$$
当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。例如，极大似然估计（MLE）就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等于极大似然估计。 	
但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生过拟合现象。而结构风险最小化（structural risk minimization, SRM）是为了防止过拟合而提出的策略。结构风险最小化等价于正则化。结构风险在经验风险的基础上加上表示模型复杂度的正则化项。在假设空间、损失函数以及训练集确定的情况下，结构风险的定义是：
$$
R_{emp}(f)=\frac{1}{N}\sum_i^NL(y_i,f(x_i))+\lambda	J(f)
$$
其中，$J(f)$为模型的复杂度，是定义在假设空间上的泛函数。模型$f$越复杂，复杂度$J(f)$就越大。也就是说，复杂度表示了对复杂模型的惩罚。结构风险小的模型往往对训练数据和未知的测试数据都有较好的预测。比如，贝叶斯估计中的最大后验概率估计（MAP）就是结构风险最小化的例子。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。
结构风险最小化的策略认为结构风险最小的模型是最优的模型。所以求解模型，就是求解最优化问题：
$$
\min_{f \in \mathcal(F)}\frac{1}{N}\sum_i^NL(y_i,f(x_i))+\lambda	J(f)
$$

3. 算法

算法是指学习模型的具体计算方法。

# 模型评估与模型选择

## 训练误差与测试误差

假设学习到的模型是$Y=\hat{f}(X)$，训练误差是模型关于训练数据集的平均损失：
$$
R_{emp}(\hat{f})=\frac{1}{N}\sum_i^NL(y_i,\hat{f}(x_i))
$$
其中N是训练样本容量

测试误差是模型关于测试数据集的平均损失：
$$
e_{test}=\frac{1}{N^‘}\sum_i^{N^‘}L(y_i,\hat{f}(x_i))
$$
以损失函数为0-1损失为例：

误差率（error rate）
$$
e_{test}=\frac{1}{N^‘}\sum_i^{N^‘}I(y_i \neq \hat{f}(x_i))
$$
准确率（accuracy）
$$
r_{test}=\frac{1}{N^‘}\sum_i^{N^‘}I(y_i = \hat{f}(x_i))
$$
显然：
$$
r_{test}+e_{test}=1
$$

## 过拟合与模型选择

当假设空间含有不同复杂度（例如，不同的参数个数）的模型时，就要面临**模型选择（model selection）**的问题。

如果在假设空间中存在“真”模型，那么我们选择的模型应该逼近“真”模型。具体的说，所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数向量相近。

如果一味的追求提高训练数据的预测能力，所选择的模型往往会比真模型复杂度更高。这种现象称为**过拟合（over-fitting）**。过拟合是指学习时所选择的模型包含的参数过多，以至于出现这一模型对已知数据预测的很好，但对未知数据预测的很差。

## 正则化与交叉验证

正则化一般具有如下形式：
$$
\min_{f \in \mathcal(F)}\frac{1}{N}\sum_i^NL(y_i,f(x_i))+\lambda	J(f)
$$
第一项是经验风险，第二项是正则化项$\lambda\geq0$为调整二者关系的系数。	

正则化项可以取不同的形式，回归问题中，损失函数是平方损失，正则化项可以是参数向量的$L_2$范数：
$$
L(w)=\frac{1}{N}\sum_i^{N}(f(x_i;w)-y_i)^2+\frac{\lambda}{2}||w||^2
$$
正则化项也可以是参数向量的$L_1$范数:
$$
L(w)=\frac{1}{N}\sum_i^{N}(f(x_i;w)-y_i)^2+{\lambda}||w||_1
$$
**交叉验证**

1. 简单交叉验证

简单交叉验证方法是：首先随机地将数据分为两部分——训练集(70%)和测试集(30%)；然后用训练集在各种条件下训练得到不同的模型，在测试集上评价各个模型的测试误差，选择测试误差最小的模型。

2. S折交叉验证

应用最多的是$S$折交叉验证（S-fold cross validation），首先随机地将数据分为$S$个互不相交的大小相同的子集，然后利用$S-1$个子集的数据进行训练，用剩下的子集进行测试；重复上述过程，最后选出$S$次测试中平均测试误差最小的模型。

3. 留一交叉验证

S折交叉验证特殊情况是$S=N$，称为留一交叉验证（leave-one cross validation），往往在数据缺乏的情况下使用。

# 泛化能力

## 泛化误差

学习方法的泛化能力（generalization ability）是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上最重要的性质。通常采用测试误差来评价学习方法的泛化能力，但这种方法依赖于测试数据，但数据较少时评价结果有可能不可靠。统计学试图从理论上对学习方法的泛化能力进行分析。

首先给出泛化误差的定义，如果学到的模型是$\hat{f}$，那么用这个模型对未知数据预测的误差即为泛化误差（generalization error）。
$$
R_{exp}\hat{f}=E_p[L(Y,\hat{f}(X))]=\int_{x \times y}L(y,\hat{f}(x))P(x,y)dxdy
$$

## 泛化误差上界

学习方法的泛化能力分析往往是通过研究误差的概率上界进行的，简称为泛化误差上界（generalization error bound）。泛化误差上界通常具有以下性质：它是样本容量的函数，当样本容量增加时，泛化上界趋向于0；它是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。

> 训练误差小的模型，其泛化误差也会小。

# 生成模型与判别模型

监督学习方法可分为生成方法（generative approach）和判别方法（discriminative approach）。

所学到的模型分别称为生成模型（generative model）和判别模型（discriminative model）。

生成方法由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型：
$$
P(Y|X)=\frac {P(X,Y)}{P(X)}
$$
之所以称为生成方法，是因为模型表示了给定输入$X$产生输出$Y$的生成关系。典型的生成模型有：朴素贝叶斯法和隐马尔可夫模型。

判别方法由数据直接学习决策函数f(X)或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。判别方法关心的是对给定的输入$X$，应该预测什么样的输出$Y$。典型的判别模型包括：k近邻法、感知机、决策树、逻辑回归模型、最大熵模型、支持向量机、提升方法和条件随机场。

在监督学习中，生成方法和判别方法各有优缺点，适合于不同条件下的学习问题。生成方法的特点：生成方法可以还原出联合概率分布$P(X,Y)$，而判别方法则不能；生成方法的收敛速度更快，当存在隐变量时，仍可以使用生成方法，此时判别方法不可用。判别方法的特点：判别方法直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，直接面对预测，往往学习的准确率更高；由于直接学习$P(Y|X)$或$f(X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。



# 分类问题

分类是监督学习的一个核心问题。在监督学习中，当输出变量$Y$取有限个离散值时，预测问题便称为分类问题。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器（classifier）。分类器对新的输入进行输出的预测（prediction），称为分类（classification）。分类的类别为多个时，称为多分类问题。	

分类问题包括学习和分类两个过程。学习过程中，根据已知的训练数据集学习一个分类器，分类过程中，根据学习的分类器对新实例进行分类。

TP——将正类预测为正类数	
FN——将正类预测为负类数		
FP——将负类预测为正类数		
TN——将负类预测为负类数	

精确率定义：
$$
P=\frac{TP}{TP+FP}
$$


召回率：
$$
P=\frac{TP}{TP+FN}
$$
$F_1$：
$$
F_1 = \frac{2TP}{2TP+FP+FN}
$$


# 标注问题

标注（tagging）也是一个监督学习问题，可以认为标注问题是分类问题的一个推广，标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。



# 回归问题

回归（regression）是监督学习的另一个重要问题。回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生变化。回归模型正是表示从输入变量到输出变量之间的映射的函数。回归问题等价于函数拟合，选择一条函数曲线使其很好地拟合已知数据且很好的预测未知数据。



















































