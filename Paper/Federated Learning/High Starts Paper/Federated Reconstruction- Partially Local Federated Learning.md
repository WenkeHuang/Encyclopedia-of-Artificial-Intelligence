## Problems

联邦学习中的个性化方法旨在平衡联合培训和本地培训在数据可用性，通信成本以及对客户端异构性的鲁棒性方面的优势。 **由于隐私和通信限制**，要求客户传达所有模型参数的方法可能是不受欢迎的。 **其他方法需要始终可用或有状态的客户端**，这在大规模跨设备设置中是不可行的。

Partially local federated learning

在这种设定下，模型被partitioned分成了全局的$g$，和局部模型$l$ (此部分不离开用户)

重要的是，这些方法无法实际应用到跨设备设置中，因为它们假定客户端是有状态的或始终可用：在实践中，客户端是从庞大的群体中采样而来的，它们的可用性不可靠，因此，这些方法依赖于相同对象的重复采样 有状态的客户是不切实际的。

这样就可以像在协作过滤设置中**一样对敏感的用户特定参数进行训练**，并且我们展示了它还可以提高对客户端数据异构性的鲁棒性和其他设置的通信成本，因为我们可以有效地在本地和联邦训练之间进行内插。 

重要的是，这些方法无法实际应用到跨设备设置中，因为它们假定客户端是有状态的或始终可用：在实践中，客户端是从庞大的群体中采样而来的，它们的可用性不可靠，因此，**这些方法依赖于相同对象的重复采样 有状态的客户是不切实际的。**

新方法应该做到：

1. Model-agnostic: works with any model. 
2.  Scalable: compatible with large-scale cross-device training.
3. Practical for inference: new clients can perform infer- ence.
4. Fast: clients can quickly adapt local parameters to their personal data.

## Idea

我们提出此框架通过考虑模型不可知的元学习，并通过经验证明其优于现有方法的性能（进行协同过滤和下一词预测），并发布一个开放源代码库以评估这种情况下的方法。 我们还描述了此方法在移动键盘应用程序中针对联合协作过滤的大规模成功部署。

## Conclusion

我们介绍了联邦重建，这是一种模型不可知的框架，适用于快速局部局部联邦学习，适合大规模培训和推理。 我们通过与元学习的连接来证明FEDRECON的合理性，并通过经验验证了用于协同过滤和下一则消息预测的算法，表明该算法可以提高对看不见的客户端的性能，并能以较少的通信实现快速的个性化。 我们还发布了一个开源库，用于部分本地联合学习，并描述了成功的生产部署。 未来的工作可能会探索局部和全局参数的最佳平衡，以及差异性隐私对全局参数的应用。