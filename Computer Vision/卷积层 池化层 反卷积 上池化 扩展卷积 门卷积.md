# 卷积层
## 定义
首先需要了解图像是个什么东西？
通道 常用于表示图像的某种组成。一个标准数字相机拍摄的图像会有**三通道** - 红、绿和蓝；你可以把它们看作是互相堆叠在一起的二维矩阵（每一个通道代表一个颜色），每个通道的像素值在 0 到 255 的范围内。
灰度图像，仅仅只有一个通道。在本篇文章中，我们仅考虑灰度图像，这样我们就只有一个二维的矩阵来表示图像。矩阵中各个像素的值在 0 到 255 的范围内——**零表示黑色，255 表示白色**。

卷积的**主要目的是为了从输入图像中提取特征**。卷积可以通过从输入的一小块数据中学到图像的特征，并可以保留像素间的空间关系。我在这里并不会详细讲解卷积的数学细节，因为我也不会，但我会试着理解卷积是如何处理图像的。

每张图像都可以看作是像素值的矩阵。考虑一下一个 5 x 5 的图像，它的像素值仅为 0 或者 1（注意对于灰度图像而言，像素值的范围是 0 到 255，下面像素值为 0 和 1 的绿色矩阵仅为特例）：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404222214515.png#pic_center)
那还有一个 3 x 3 的矩阵，如下所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020040422230071.png#pic_center)
他们结合在了一起，得到了如下的结果，操作的方式可以理解成每个黄色的值都和绿色的值相乘，结果相加：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404222324444.png#pic_center)
在 CNN 的术语中，3x3 的矩阵叫做“滤波器（filter）”或者“核（**kernel**）”或者“特征检测器（feature detector）”，通过在图像上滑动滤波器并计算点乘得到矩阵叫做“卷积特征（Convolved Feature）”或者“激活图（Activation Map）”或者“特征图（Feature Map）”。记住滤波器在原始输入图像上的作用是特征检测器。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404222750306.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNDA5NDM4,size_16,color_FFFFFF,t_70#pic_center)
## 特点
- 局部感知：人的大脑识别图片的过程中，并不是一下子整张图同时识别，而是对于图片中的每一个特征首先局部感知，然后更高层次对局部进行综合操作，从而得到全局信息。第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。
- 参数共享
五个小朋友对应五个神经元，也叫filter，每个小盆友视野有限，每次只看一小块，慢慢滑动直到看完，那么左图中的小块叫感受野（receptive field）。
每个小朋友看完一张图得到一张图，五个人得出五张，即depth是5，即神经元个数。
小盆友1视野很小，每个感受野对应一些参数，假设为w1w2w3w4,每滑动一次，另一个感受野又对应四个w,因为每个小朋友都有自己处事原则（不管看什么，参数不变），所以一个小盆友只要学习四个参数。一幅图只要4*5=20个参数。

# 池化层
池化（Pooling）是卷积神经网络中另一个重要的概念，它实际上是一种形式的**降采样**。有多种不同形式的非线性池化函数，而其中“**最大池化（Max pooling）**”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的卷积层之间都会周期性地插入池化层。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404225750875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNDA5NDM4,size_16,color_FFFFFF,t_70#pic_center)
## 特点
1. 特征不变形：池化操作是模型更加关注是否存在某些特征而不是特征具体的位置。
2. 特征降维：池化相当于在空间范围内做了维度约减，从而使模型可以抽取更加广范围的特征。同时减小了下一层的输入大小，进而减少计算量和参数个数。
3. 在一定程度上防止过拟合，更方便优化。

# 上采样
在应用在计算机视觉的深度学习领域，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(e.g.:图像的语义分割)，这个采用扩大图像尺寸，实现图像由小分辨率到大分辨率的映射的操作，叫做上采样(Upsample)。
上采样有3种常见的方法：双线性插值(bilinear)，反卷积(Transposed Convolution)，反池化(Unpooling)。
# 反卷积/转置卷积
反卷积也被称为转置卷积，反卷积其实就是卷积的逆过程。大家可能对于反卷积的认识有一个误区，以为通过反卷积就可以获取到经过卷积之前的图片，实际上通过反卷积操作并不能还原出卷积之前的图片，只能还原出卷积之前图片的尺寸。
[Link]http://www.360doc.com/content/19/0507/12/57110788_834069126.shtml

# 上采样
池化也叫下采样(down-sampling), 操作与普通卷积基本相同, 不过根据取最大值或平均值可分为最大池化和平均池化, 同时无反向传播过程(无需学习参数).

**上池化保留位置信息补0**, **上采样不保留位置信息直接复制**. 但二者均无反向传播过程(无需学习参数), 也就是对中间地带不采取过渡值只是简单处理.
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020040523114652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNDA5NDM4,size_16,color_FFFFFF,t_70#pic_center)
- 图（a）表示上池化UnPooling的过程，特点是在池化Maxpooling的时候保留最大值的位置信息，之后在上池化UnPooling阶段使用该信息扩充Feature Map，除最大值位置以外，其余补0。
- 与之相对的是图（b），两者的区别在于上采样UnSampling阶段没有使用MaxPooling时的位置信息，而是直接将内容复制来扩充Feature Map。从图中即可看到两者结果的不同。
- 图（c）为反卷积的过程，反卷积是卷积的逆过程，又称作转置卷积(Transposed Convolution)。最大的区别在于反卷积过程是有参数要进行学习的（类似卷积过程），而上池化和上采样是无反向传播过程的. 理论上反卷积可以实现UnPooling和unSampling，只要卷积核的参数设置的合理。

# 扩展卷积 dilated convolution
## 区别
扩张卷积和普通卷积的区别在于，卷积核的大小是一样的，在神经网络中即参数数量不变，区别在于扩张卷积具有更大的感受野。感受野是卷积核在图像上看到的大小。
相比原来的标准卷积，扩张卷积（dilated convolution） 多了一个hyper-parameter（超参数）称之为dilation rate（扩张率），指的是kernel各点之前的间隔数量，正常的convolution 的 dilatation rate为 1。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200409113133694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNDA5NDM4,size_16,color_FFFFFF,t_70#pic_center)

(a)图对应3x3的1-dilated conv，和普通的卷积操作一样。(b)图对应3x3的2-dilated conv，实际的卷积kernel size还是3x3，但是空洞为1，**需要注意的是空洞的位置全填进去0**，填入0之后再卷积即可。
比如上图中（a），dilated=1，F(dilated) = 3×3；图（b）中，dilated=2，F(dilated)=7×7；图（c）中，dilated=4， F(dilated)=15×15。
dilated=2时具体的操作，即按照下图在空洞位置填入0之后，然后直接卷积就可以了。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200409113848893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNDA5NDM4,size_16,color_FFFFFF,t_70#pic_center)
## 空洞卷积的动态过程
![在这里插入图片描述](https://img-blog.csdn.net/20180908172420316?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoZW55dXBpbmczMzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70#pic_center)

 上图是一个扩张率为2的3×3卷积核，感受野与5×5的卷积核相同，而且仅需要9个参数。你可以把它想象成一个5×5的卷积核，每隔一行或一列删除一行或一列。
在相同的计算条件下，空洞卷积提供了更大的感受野。空洞卷积经常用在实时图像分割中。当网络层需要较大的感受野，但计算资源有限而无法提高卷积核数量或大小时，可以考虑空洞卷积。

## Dilated Convolution感受野指数级增长

对于标准卷积核情况，比如用3×3卷积核连续卷积2次，在第3层中得到1个Feature点，那么第3层这个Feature点换算回第1层覆盖了多少个Feature点呢？
第一层的一个5×5大小的区域经过2次3×3的标准卷积之后，变成了一个点。也就是说从size上来讲，2层3*3卷积转换相当于1层5*5卷积。即，一个5×5的卷积核是可以由2次连续的3×3的卷积代替。
但对于dilated=2，3*3的扩张卷积核呢？
可以看到第一层13×13的区域，经过2次3×3的扩张卷积之后，变成了一个点。即从size上来讲，连续2层的3×3空洞卷积转换相当于1层13×13卷积
# Gated Convolution

## Why
- vanilla convolution(普通卷积)认为所有通道的输入像素都是有效的，但gated convolution可以通过对每个channel应用参数可学的特征选择机制，将所有层响应空间位置的像素融合到一起，达到对partial convolution(部分卷积)进行泛化的目的。
- 针对规则的矩形mask设计出来的GANs无法处理free-form masks，已经有文章论证了vanilla convolution无法解决free-form image inpainting任务。
## How
partial conv可以被认为是hard-gating single-channel un-learnable layer再跟input feature map逐像素点乘。
gated conv抛弃通过固定规则进行更新的hard mask，而是从数据中自动学习soft mask，公式如下：
$$
Gating_{y,x} = \sum \sum W_g \cdot I \\
Feature_{y,x} = \sum \sum W_f \cdot I \\
O_{y,x}  = \Phi(Feature_{y-x})\odot\sigma(Gating_{y,x})
$$

 其中，σsigmaσ表示对0～1的output gating values使用sigmoid激活函数，ϕ可以是任意激活函数（ReLU or LeakyReLU）