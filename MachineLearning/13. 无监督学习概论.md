# 基本原理

- 无监督学习是从无标注的数据中学习数据的统计规律或者说内在结构的机器学习，主要包括**聚类、降维、概率估计**
- 无监督学习的基本想法是对给定数据（矩阵数据）进行某种“压缩”，从而找到数据的潜在结构。**假定损失最小的压缩得到的结果就是最本质的结构**

发掘数据的*纵向结构*，把相似的样本聚到同类，即对数据进行**聚类**

![Cluster](../img/ML/Cluster.png)

发掘数据的*横向结构*，把高维空间的向量转换为低维空间的向量，即对数据进行**降维**聚类

![Dimensionalityreductioncluster](../img/ML/Dimensionalityreductioncluster.png)

同时发掘数据的纵向和横向结构，假设数据由含有隐式结构的概率模型生成得到，从数据中**学习该概率模型**

![Horizontalandvertical](../img/ML/Horizontalandvertical.png)

# 三个基本问题

## 聚类

- 聚类（clustering）是将样本集合中相似的样本（实例）分配到相同的类，不相似的样本分配到不同的类。
- 聚类时，样本通常是欧氏空间中的向量，类别不是预先给定，而是从数据中自动发现，但类别的个数通常是预先给定的。样本之间的相似度或距离由 应用决定。





如果一个样本只能属于一个类，则称为硬聚类（硬聚类）
$$
z_i = g_\theta(x_i), i = 1,2,...,N
$$
如果一个样本可以属于多个类，则称为软聚类（软聚类）
$$
P_\theta(z_i|x_i), i = 1,2,...,N
$$

## 降维

- 降维（dimensionality reduction）是将训练数据中的样本（实例）从高维空间转换到低维空间
- 假设样本原本存在于低维空间，或者近似地存在于低维空间，通过降维则可以更好地表示样本数据的结构，即更好地表示样本之间的关系
- 高维空间通常是高维的欧氏空间，而低维空间是低维的欧氏空间或者流形（manifold)
- 从高维到低维的降维中，要保证样本中的信息损失最小

![dimensionalityreduction](../img/ML/dimensionalityreduction.png)

降维的模型是函数

```
z_i = g_\theta(x_i), i = 1,2,...,N
```

其中$x_i\in X$是样本的高维向量，$z_i \in Z$是样本的低维向量，$\theta$是参数，函数可以是线性函数也可以是非线性函数，降维的过程就是学习降维模型的过程

## 概率模型估计

- 概率模型估计（probility model estimation），简称概率估计，假设训练数据由一个概率模型生成，由训练数据学习概率模型的结构和参数。
- 概率模型的**结构类型**（或者说概率模型的集合事先给定），而模型的**具体结构与参数从数据中自动学习**。学习的目标是找到最有可能生成数据的结构和参数。

![probilitymodelestimation](../img/ML/probilitymodelestimation.png)

概率模型表示为条件概率分布
$$
P_\theta(x|z)
$$
随机变量$x$表示观测数据，可以是连续变量也可以是离散变量，随机变量$z$表示隐式结构，是离散变量，随机变量$\theta$表示参数。概率模型的一种特殊情况是隐式结构不存在，即满足
$$
P_\theta(x|z) = P_\theta(x)
$$
这时条件 概率分布估计变成概率分布估计，只要估计分布$P_\theta(x)$的参数即可

# 机器学习三要素

(1)模型: 

函数：
$$
z = g_\theta(x)
$$
条件概率分布：
$$
P_\theta(z|x)
$$
条件概率分布
$$
P_\theta(x|z)
$$
(2)策略: 目标函数的优化

(3)算法: 迭代算法，通过迭代达到对目标函数的最优化









