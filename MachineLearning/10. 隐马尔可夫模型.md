# 隐马尔可夫模型

关键词：直接计算法、前向算法、后向算法、学习算法，预测算法

# 隐马尔可夫模型的基本概念

 ## 隐马尔可夫模型的定义

Q是所有可能的状态的集合：
$$
Q = \{q_1,q_2,...,q_N\}
$$
V是所有可能的观测的集合：
$$
V= \{v_1,v_2,...,v_m\}
$$
N是可能的状态数，M是可能的观测数。

I是长度为T的状态序列。
$$
I=(i_1,i_2,....,i_T)
$$
O是对应的观测序列。
$$
Q=(o_1,o_2,....,o_T)
$$
A是状态转移概率矩阵：
$$
A = [a_{ij}]_{M\times N}
$$
其中：
$$
a_{ij}=P(i_{t+1}=q_j|i_t=q_i),i=1,2,...,N;j=1,2,...,N
$$
B是观测概率矩阵：
$$
B=[b_j(k)]_{N \times M}
$$
其中：
$$
b_j(k)=P(o_t=v_k|i_t=q_j)，k=1,2,...,M;j=1,2,...,M
$$


$\pi$是初始状态概率向量：
$$
\pi = (\pi_i)\\
\pi_i=P(i_1=q_i),i=1,2,...N\\
是时刻t=1处于状态q_i的概率
$$
隐马尔可夫模型
$$
\lambda=(A,B,\pi)
$$
**两个基本假设**

1. 齐次马尔可夫行假设：即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关。
2. 观测独立性假设，即假设任意时刻的观测值的观测值只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。

## 隐马尔可夫模型的三个基本问题

1. 概率计算问题：

$$
给定模型\lambda=(A,B,\pi)和观测序列O=(o_1,o_2,...,o_T)，
计算在模型\lambda下观测序列O出现的概率P(O|\lambda)
$$

2. 学习问题：

$$
已知观测序列O=(o_1,o_2,...,o_T)，估计模型\lambda=(A,B,\pi,)参数，使得该模型下观测序列概率P(O|\lambda)最大，即用极大似然估计的方法估计参数
$$

3. 预测问题：
   $$
   已知模型\lambda=(A,B,\pi)和观测序列O=(o_1,o_2,...,o_T)，求对给定观测序列条件概率P(I|O)最大状态序列I=(i_1,i_2,...,i_T)，即给定观测序列，求最有可能的对应的状态序列。
   $$



# 概率计算算法

## 直接计算法

给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,...,o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率P$(O|\lambda)$

状态序列I=(i_1,i_2,...,i_T)的概率：
$$
P(I|\lambda)=\pi_{i_1}a_{i_1i_2}a_{i_2i_3}...a_{i_{T-1}i_T}
$$
对固定状态序列，得到对应的观测序列的概率：
$$
P(O|I,\lambda)=b_{i_1}(o_1)b_{i_2}(o_2)....b_{i_T}(o_T)
$$
$O和I$同时出现的联合概率为：
$$
P(O,I|\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)a_{i_2i_3}...a_{i_{T-1}i_T}b_{i_T}(o_T)
$$
对所有的可能的状态序列I求和：
$$
P(O|\lambda) = \sum_I P(O|I,\lambda)P(I|\lambda)
$$
计算量很大，$O(TN^T)$阶

## 前向算法

给定隐马尔可夫模型$\lambda$，定义到时刻$t$部分观测序列为$o_1,o_2,...,o_t$，且状态为$q_i$的概率为前向概率，记作：
$$
\alpha_t(i)=P(o_1,o_2,....,o_t,i_t=q_i|\lambda)
$$
初值：
$$
\alpha_1=\pi_ib_i(o_1),i=1,2,...,N
$$
递推：
$$
\alpha_{t+1}(i)=[\sum_{j=1}^{N}\alpha_t(j)a_{ji}]b_i(o_{t+1})
$$
终止：
$$
P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)
$$

## 后向算法

给定隐马尔可夫模型$\lambda$，定义在时刻$t$状态为$q_i$的条件下，从t+1到T的部分观测序列为$o_{t+1},o_{t+2},...o_T$的概率为后向概率，记作：
$$
\beta_t(i)=P(o_{t+1},o_{t+2},...,o_T|)
$$
Step1:
$$
\beta_T(i)=1，i=1,2,...,N
$$
Step2 $t=T-1,T-2,...,1$：
$$
\beta_t(i)=\sum_{j=1}^Na_{ij}b_j(o_{t+1})\beta_{t+1}(j),i=1,2,...,N
$$
Step3：
$$
P(O|\lambda)=\sum_{i=1}^N\pi_ibi(o_1)\beta_1(i)
$$

## 一些概率与期望值的计算

1. 给定模型$\lambda$和观测$O$，在时刻$t$处于状态$q_i$的概率：
   $$
   \gamma_t(i)=P(i_t=q_i|O,\lambda) \\
   =\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}
   $$

2. 给定模型$\lambda和观测$O$，在时刻$t$处于状态$q_i$且在时刻$t+1$处于状态$q_j$的概率，记作：

$$
\zeta_t(i,j)=\frac{\alpha_t(i)\beta_{t+1}(j)a_{ij}b_j(o_{t+1})}{\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)\beta_t(j)a_{ij}b_j(o_{t+1})}
$$

3. 概率值求和：

(1)在观测O在状态i出现的期望值：
$$
\sum_{t=1}^T\gamma_t(i)
$$
(2)在观测O下由状态i转移的期望值
$$
\sum_{t=1}^{T-1}\gamma_t(i)
$$
(3)在观测O下由状态$i$转移到状态$j$的期望值：
$$
\sum_{t=1}^{T-1}\zeta_t(i,j)
$$

# 学习算法

## 监督学习方法

1. 转移概率估计$a_{ij}$：_

   设样本中时刻$t$处于状态$i$时刻$t+1$转移到状态$j$的频数$A_{ij}$，那么状态转移概率$a_{ij}$的估计是：

$$
\widehat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^NA_{ij}}
$$

2. 观测概率$b_j(k)$：

设样本状态为$j$并观测为$k$的频数是$B_{jk}$，那么状态为$j$观测为$k$的概率$b_j(k)$的估计是
$$
\widehat{b_j}(k)=\frac{B_{jk}}{\sum_{k=1}^MB_{jk}}
$$

3. 初始状态概率$\pi_i$的估计为$S$个样本中初始状态为$q_i$的频率

## Baum-Welch算法

$$
P(O|\lambda) = \sum_IP(O|I,\lambda)P(I|\lambda)
$$

Step1 确定完全数据的对数似然函数：

所有观测数据写成$O=(o_1,o_2,....,o_T)$，所有隐数据写成$I=(i_1,i+2,....,i_T)$，完全数据的对数似然函数$logP(O,I|\lambda)$

Step2 EM算法的E步：求$Q$函数

Q(\lambda,\overline{\lambda})=\sum_IlogP(O,I|\lambda)P(O,I|\overline{\lambda})

其中，$\overline{\lambda}$是隐马尔可夫模型参数的当前估计值，$\lambda$是要极大化的隐马尔可夫模型参数：
$$
P(O,I|\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}b_{i_T}(o_T)
$$
于是可以改写Q函数：
$$
Q(\lambda,\overline{\lambda})=\sum_I(log\pi_{i_1})P(O,I|\overline{\lambda})+\sum_I(\sum_{t=1}^{T-1}loga_{i_ti_{t+1}})P(O,I|\overline{\lambda})+\sum_I(\sum_{t=1}^Tlogbi_t(o_t))P(O,I|\overline{\lambda})
$$
Step3 EM算法的M步：极大化Q函数，求模型参数$A、B、\pi$：

由于要极大化的参数在上式中单独地出现在3个项中，所以只需要对各项分别极大化。
$$
\sum_Ilog\pi_{i_0}P(O,I|\lambda)=\sum_{i=1}^Nlog\pi_iP(O,i_1=i|\overline{\lambda})
$$
注意到$\pi_i$满足约束条件$\sum_{i=1}^N\pi_i=1$，利用拉格朗日乘子法，写出拉格朗日函数：
$$
\sum_{i=1}^Nlog\pi_iP(O,i_1=i|\overline{\lambda})+\gamma(\sum_{i=1}^N\pi_i-1)
$$
之后求偏导数并且令结果为0

得到：
$$
\pi_i = \frac{P(P,i_1=i|\overline{\lambda})}{P(O|\overline{\lambda})}
$$
还有：
$$
a_{ij}=\frac{\sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\overline{\lambda})}{\sum_{t=1}^{T-1}P(O,i_t=i|\overline{\lambda})}
$$
以及：
$$
b_j(k)=\frac{\sum_{t=1}^TP(O,i_t=j|\overline{\lambda})I(o_t=v_k)}{\sum_{t=1}^TP(P,i_t=j|\overline{\lambda})}
$$

# 预测算法

## 近似算法

在每个时刻选择在该时刻最有可能出现的状态从而得到一个状态序列：
$$
\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_tji)}
$$
在每一时刻最有可能的状态是：
$$
i_t^* = arg \max_{1\leq i \leq N}[\gamma_t(i)]，t=1,2,..,T
$$
从而得到了对应的状态序列；计算简单，但是不能保证预测的状态序列整体是最有可能的状态序列。

## 维特比算法

动态规划求概率最大路径：

定义在时刻$t$状态为$i$的所有单个路径$(i_1,i_2,...,i_t)$中概率最大值为
$$
\delta_t(i)=\max_{i_1,i_2,..,i_{t-1}}P(i_t=i,i_{t-1},...,i_1,o_t,...,o_1|\lambda),i=1,2,...,N
$$
由定义得到了递推公式
$$
\delta_{t+1}(i)=\max_{1\leq j\leq N}[\delta_t(j)a_{ji}]b_i(o_{t+1})
$$
定义在时刻$t$状态为$i$的所有单个路径$(i_1,i_2,...,i_{t-1},i)$中概率最大的路径的第$t-1$个结点为
$$
\Psi_t(i)=arg\max_{1\leq j\leq N}[\delta_{t-1}(j)a_{ji}],i=1,2,...,N
$$




















































